{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e754d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c43018-95c9-49df-b34d-074efb86f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from a pickle file into a DataFrame\n",
    "data = pd.read_pickle('./predictions/all_primary_10.pkl')\n",
    "\n",
    "# Optionally rename a column if needed (currently commented out)\n",
    "# data = data.rename(columns={'pred_lqhq_data3' : 'predictions'})\n",
    "\n",
    "# Print the column names of the DataFrame to understand the structure of the loaded data\n",
    "print(data.columns)\n",
    "\n",
    "# Define a list of categories\n",
    "categories = ['ADM', 'ATT', 'BER', 'ENR', 'ETN', 'FAC', 'INS', 'MBW', 'STM', 'O']\n",
    "\n",
    "# Create a dictionary mapping each category to its index\n",
    "cat_indices = {category: index for index, category in enumerate(categories)}\n",
    "\n",
    "# Iterate over each category and its corresponding index\n",
    "for cat, index in cat_indices.items():\n",
    "    # Create a new column in the DataFrame for each category, containing only the predictions\n",
    "    # The predictions are extracted based on the index of the category in the 'pred_jenia_M3' column\n",
    "    data[cat] = data['pred_jenia_M3'].apply(lambda x: x[index])\n",
    "    \n",
    "    # Create a new column in the DataFrame for each category's confidence scores\n",
    "    # The confidence scores are extracted based on the index of the category in the 'confidence_jenia_M3_ft' column\n",
    "    data[f'{cat}_confidence'] = data['confidence_jenia_M3_ft'].apply(lambda x: x[index])\n",
    "\n",
    "# Filter the DataFrame to create new DataFrames for each category where the prediction is 1 (i.e., the category is predicted)\n",
    "# For inspection if needed\n",
    "only_adm = data[data['ADM'] == 1]\n",
    "only_att = data[data['ATT'] == 1]\n",
    "only_ber = data[data['BER'] == 1]\n",
    "only_enr = data[data['ENR'] == 1]\n",
    "only_etn = data[data['ETN'] == 1]\n",
    "only_fac = data[data['FAC'] == 1]\n",
    "only_ins = data[data['INS'] == 1]\n",
    "only_mbw = data[data['MBW'] == 1]\n",
    "only_stm = data[data['STM'] == 1]\n",
    "only_o = data[data['O'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543f6b3-280c-48b3-b6c6-4ebe2056dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with categories as columns and statistics as index\n",
    "stats_df = pd.DataFrame(index=['Mean','Max','Min','Median'], columns=categories)\n",
    "\n",
    "# Iterate over each category to calculate the required statistics\n",
    "for cat in categories:\n",
    "    # Define the name of the confidence column for the current category\n",
    "    conf_col = f\"{cat}_confidence\"\n",
    "    \n",
    "    # Check if both the category column and its corresponding confidence column exist in the DataFrame\n",
    "    if cat in data.columns and conf_col in data.columns:\n",
    "        # Filter the DataFrame to include only the rows where the current category is predicted as 1\n",
    "        filtered_df = data[data[cat] == 1]\n",
    "\n",
    "        # Calculate and store the mean of the confidence scores for the current category\n",
    "        stats_df.loc['Mean', cat] = filtered_df[conf_col].mean()\n",
    "        \n",
    "        # Calculate and store the maximum of the confidence scores for the current category\n",
    "        stats_df.loc['Max', cat] = filtered_df[conf_col].max()\n",
    "        \n",
    "        # Calculate and store the minimum of the confidence scores for the current category\n",
    "        stats_df.loc['Min', cat] = filtered_df[conf_col].min()\n",
    "        \n",
    "        # Calculate and store the median of the confidence scores for the current category\n",
    "        stats_df.loc['Median', cat] = filtered_df[conf_col].median()\n",
    "\n",
    "# Transpose the DataFrame to have categories as rows and statistics as columns\n",
    "stats_df = stats_df.transpose()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404aaea4-3eea-449d-b7e2-27841f7d7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set pandas option to display all columns when printing DataFrames (currently commented out)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the desired confidence range\n",
    "conf_max = 0.23  # Set desired confidence upper limit\n",
    "conf_min = 0.19  # Set desired confidence lower limit\n",
    "cat = 'ATT'      # Set the category of interest\n",
    "\n",
    "# Filter the DataFrame to include only rows where the confidence for the specified category falls within the desired range\n",
    "filtered_conf_df = data[(data[f'{cat}_confidence'] >= conf_min) & (data[f'{cat}_confidence'] <= conf_max)]\n",
    "\n",
    "# Sum the values for each category in the filtered DataFrame to get the total number of predictions for each category\n",
    "conf_categories = filtered_conf_df[categories].sum()\n",
    "\n",
    "# Further filter the DataFrame to include only rows where the specified category is positively predicted (i.e., prediction is 1)\n",
    "only_positive_filtered_conf_df = filtered_conf_df[filtered_conf_df[cat] == 1]\n",
    "\n",
    "# Display the resulting filtered DataFrame containing only positive predictions within the confidence range for the specified category\n",
    "only_positive_filtered_conf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3113e-c222-4691-a754-71278ba711c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionalized version of the operation above to apply in bulk\n",
    "\n",
    "def get_instances(data, category, max_conf, min_conf):\n",
    "    \"\"\"\n",
    "    Function to filter instances based on confidence scores for a specified category,\n",
    "    and return a DataFrame of unique instances with specific columns.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input DataFrame containing the data.\n",
    "    category (str): The category of interest for filtering based on confidence scores.\n",
    "    max_conf (float): The maximum confidence threshold.\n",
    "    min_conf (float): The minimum confidence threshold.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing unique instances with specific columns.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include only rows where the confidence for the specified category is within the desired range\n",
    "    filtered_conf_df = data[(data[f'{category}_confidence'] >= min_conf) & (data[f'{category}_confidence'] <= max_conf)]\n",
    "    \n",
    "    # Calculate the sum of values for each category in the filtered DataFrame\n",
    "    conf_categories = filtered_conf_df[categories].sum()\n",
    "    \n",
    "    # Further filter the DataFrame to include only rows where the specified category is positively predicted (i.e., prediction is 1)\n",
    "    only_positive_filtered_conf_df = filtered_conf_df[filtered_conf_df[category] == 1]\n",
    "    \n",
    "    # Select specific columns and drop duplicates based on the 'text' column\n",
    "    instances = only_positive_filtered_conf_df[['NoteID', 'text', 'pred_jenia_M3']].drop_duplicates(subset=['text'])\n",
    "\n",
    "    return instances\n",
    "\n",
    "# Define the confidence range\n",
    "conf_max = 0.23  # Set desired confidence upper limit\n",
    "conf_min = 0.19  # Set desired confidence lower limit\n",
    "\n",
    "# Apply the get_instances function to each category to get unique instances within the specified confidence range\n",
    "instances_adm = get_instances(data, 'ADM', conf_max, conf_min)\n",
    "instances_att = get_instances(data, 'ATT', conf_max, conf_min)\n",
    "instances_ber = get_instances(data, 'BER', conf_max, conf_min)\n",
    "instances_enr = get_instances(data, 'ENR', conf_max, conf_min)\n",
    "instances_etn = get_instances(data, 'ETN', conf_max, conf_min)\n",
    "instances_fac = get_instances(data, 'FAC', conf_max, conf_min)\n",
    "instances_ins = get_instances(data, 'INS', conf_max, conf_min)\n",
    "instances_mbw = get_instances(data, 'MBW', conf_max, conf_min)\n",
    "instances_stm = get_instances(data, 'STM', conf_max, conf_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71b464-568e-458f-9dec-9602ba0d012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If merging individual slices above needed \n",
    "\n",
    "combined_high = pd.concat([instances_adm, instances_att, instances_ber, instances_enr, instances_etn, instances_fac, instances_ins, instances_mbw, instances_stm], axis=0, ignore_index=True)\n",
    "\n",
    "# If we want to use the merged dataset as training data, we can apply further modifications like this\n",
    "combined_high = combined_high.rename(columns={'predictions' : 'labels_10'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89196de6-6bd9-4763-b9e3-9022d8e2f870",
   "metadata": {},
   "source": [
    "### Redundancy Elimination Process\n",
    "\n",
    "- Cosine Similarity Based Clustering via DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded10b3-3d1a-496d-aa8f-cabfb6f4876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RepresentationModel with the specified model type and name, and enable CUDA for GPU usage\n",
    "model = RepresentationModel(\n",
    "    model_type=\"roberta\",\n",
    "    model_name=\"./models/jenia_M1\", # Desired model path\n",
    "    use_cuda=True\n",
    ")\n",
    "\n",
    "def remove_short_words(text):\n",
    "    \"\"\"\n",
    "    Function to remove words shorter than 4 characters from a given text.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "    str: The text with short words removed.\n",
    "    \"\"\"\n",
    "    return ' '.join([word for word in text.split() if len(word) >= 4])\n",
    "\n",
    "# Normalize the text in the 'text' column by stripping whitespace, converting to lowercase, and replacing multiple spaces with a single space\n",
    "instances['normalized'] = instances['text'].str.strip().str.lower().str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Apply the remove_short_words function to the normalized text\n",
    "instances['reduced'] = instances['normalized'].apply(remove_short_words)\n",
    "\n",
    "# Remove duplicate rows based on the 'text' column\n",
    "instances = instances.drop_duplicates(subset=['reduced'])\n",
    "\n",
    "# Encode the reduced text using the model to generate embeddings\n",
    "embeddings = model.encode_sentences(instances['reduced'].tolist(), combine_strategy=\"mean\")\n",
    "\n",
    "# Calculate the cosine distance matrix for the embeddings\n",
    "distance_matrix = cosine_distances(embeddings)\n",
    "\n",
    "# Perform DBSCAN clustering on the embeddings with specified parameters\n",
    "clustering = DBSCAN(eps=0.02, min_samples=2, metric='cosine').fit(embeddings)\n",
    "# Extract the labels assigned by the clustering algorithm\n",
    "labels = clustering.labels_\n",
    "#print(list(labels)) # Optionally print the labels\n",
    "\n",
    "\n",
    "# Assign cluster labels to the instances DataFrame\n",
    "instances['cluster'] = labels\n",
    "\n",
    "# Get the unique cluster labels\n",
    "unique_clusters = instances['cluster'].unique()\n",
    "\n",
    "# Count the number of instances in each cluster\n",
    "cluster_counts = instances['cluster'].value_counts()\n",
    "\n",
    "# Select the top 10 instances from each cluster and reset the index\n",
    "per_cluster = instances.groupby('cluster').head(10).reset_index()\n",
    "\n",
    "# Create a DataFrame containing only 'NoteID' and 'text' columns for the top 10 instances in each cluster\n",
    "clustered = per_cluster[['NoteID', 'text']]\n",
    "\n",
    "# Reduce the instances DataFrame to contain only 'NoteID' and 'text' columns\n",
    "instances = instances[['NoteID', 'text']]\n",
    "\n",
    "# Optionally reset the pandas display option for maximum rows (currently commented out)\n",
    "# pd.reset_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1e28b-682f-4b07-ae6c-4d6528a89aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'combined_high' DataFrame to a pickle file\n",
    "combined_high.to_pickle('./data/queried_data/high_conf_pseudo_m3-17k.pkl')\n",
    "\n",
    "# Save the 'instances' DataFrame to a CSV file, which can be further prepared to send for annotation\n",
    "instances.to_csv('./data/queried_data/2023_Notes_3rd_750k/notes_5th750k_att_0146-012-509-M2-ATT1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8aa73f-195f-42c3-a2b2-2b72c1c92a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This histogram shows the distribution of pairwise cosine distances between the embeddings\n",
    "#\n",
    "\n",
    "distances = pairwise_distances(embeddings, metric='cosine')\n",
    "print('Max dist:', distances.max(), 'Min dist', distances.min())\n",
    "\n",
    "plt.hist(distances.flatten(), bins=50)\n",
    "plt.title('histogram of pairwise cosine similarity')\n",
    "plt.xlabel('distance')\n",
    "plt.ylabel('freq')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
