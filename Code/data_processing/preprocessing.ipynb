{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8e788f-fadf-4be9-b069-fc81bfda8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eed3dff-57f0-440b-b488-8aef5aa2360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data\n",
    "df = pd.read_pickle('./data/jenia_data/train/high_conf_pseudo_m3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75e721cf-cc5e-49fa-97e2-8e70539f4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First inspect the raw data, assess the relevant columns and create new dataframes including only the needed information\n",
    "\n",
    "# For VUMC Notities (Notes)\n",
    "# Create a 'text' column by converting the content of column 8 to string type\n",
    "df['text'] = df[8].astype(str)\n",
    "# We can create new columns with whatever we need from the raw data\n",
    "# df['Note_ID'] = df[3].astype(str)\n",
    "\n",
    "# Create a new DataFrame 'df_note_text' by dropping the columns that are not needed\n",
    "# In this case, dropping columns 0 to 10 except for the newly created 'text' column\n",
    "\n",
    "#df_note_text = df.drop([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], axis=1)\n",
    "\n",
    "# Print the column names of the DataFrame to inspect the remaining columns\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c0a3998-e065-40ec-a926-ba404248c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the extracted dataframe which includes only relevant columns (e.g. NoteID + text) into a pickle file to refer later\n",
    "df_note_text.to_pickle('data1/2023_Notities.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "286e7f77-5d0f-4b43-8ea2-73f2a68fddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['NoteID', 'text']] # Make sure we load only relevant columns (might be redundant if done already in previous step)\n",
    "\n",
    "# Since preprocessing takes quite long for 800k or 1m notes, it would be efficient to divide the data into batches\n",
    "# Here, for example it takes the first 200k notes to process only.\n",
    "df_notes_first200 = df.iloc[0:200000].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299be49-b108-47bd-bacf-e084320c229c",
   "metadata": {},
   "source": [
    "Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "493a0317-8080-44ec-955e-cd1c94373a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to process a row of DataFrame by splitting its text into sentences\n",
    "def process_row(row):\n",
    "    \"\"\"\n",
    "    Process a row from a DataFrame by splitting its text into sentences.\n",
    "\n",
    "    Parameters:\n",
    "    row (pd.Series): A row from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of new rows, each containing an identifier and a sentence.\n",
    "    \"\"\"\n",
    "    new_rows = []\n",
    "    doc = nlp(row[8] if pd.notna(row[8]) else \"\")  # Process the text in column 8 if not NaN\n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        identifier = f'{row[1]}_{i}'  # Create an identifier based on the row's second column and sentence index\n",
    "        new_rows.append([identifier, sent.text])  # Append the identifier and sentence to the list\n",
    "    return new_rows\n",
    "\n",
    "# An alternative function to process a row of DataFrame by splitting its text into sentences\n",
    "def process_row2(row):\n",
    "    \"\"\"\n",
    "    Process a row from a DataFrame by splitting its text into sentences.\n",
    "\n",
    "    Parameters:\n",
    "    row (pd.Series): A row from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of new rows, each containing an identifier and a sentence.\n",
    "    \"\"\"\n",
    "    new_rows = []\n",
    "    doc = nlp(row['text'] if pd.notna(row['text']) else \"\")  # Process the text in 'text' column if not NaN\n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        identifier = f\"{row['NoteID']}_{i}\"  # Create an identifier based on the 'NoteID' and sentence index\n",
    "        new_rows.append((identifier, sent.text))  # Append the identifier and sentence to the list\n",
    "    return new_rows\n",
    "\n",
    "# The function to anonymize a text by replacing specific entities\n",
    "def anonymize(txt, nlp):\n",
    "    \"\"\"\n",
    "    Replace entities of type PERSON and GPE with 'PERSON' and 'GPE'.\n",
    "    Return anonymized text and its length.\n",
    "\n",
    "    Parameters:\n",
    "    txt (str): The input text.\n",
    "    nlp (Language): The spaCy language model.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the anonymized text and its length.\n",
    "    \"\"\"\n",
    "    doc = nlp(txt)  # Process the text with the spaCy language model\n",
    "    anonym = str(doc)  # Convert the doc object to a string\n",
    "    to_repl = {str(ent): ent.label_ for ent in doc.ents if ent.label_ in ['PERSON', 'GPE']}  # Identify entities to replace\n",
    "    for string, replacement in to_repl.items():\n",
    "        anonym = anonym.replace(string, replacement)  # Replace entities in the text\n",
    "    return anonym, len(doc)  # Return the anonymized text and its length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4658ce7-0c3e-4bf9-8c54-9309933f798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dutch language model from spaCy\n",
    "nlp = spacy.load('nl_core_news_lg')\n",
    "\n",
    "# Initialize tqdm to add a progress bar for pandas operations\n",
    "tqdm.pandas(desc=\"Processing rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fe5bc33-cda5-47b9-9dd8-c74cc0999fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|████████████████| 200000/200000 [1:56:15<00:00, 28.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process each row in the DataFrame to split text into sentences and explode the resulting lists into separate rows, works with tqdm process bar\n",
    "sentences_data_series = df_notes_first200.progress_apply(process_row2, axis=1).explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535ebc6-468a-4660-b4a2-a6373903b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the notes divided into sentences as series to refer later\n",
    "sentences_data_series.to_pickle('./data1/sentences_data_series.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "431d8a18-dbaf-49b3-9b8a-39edb170179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the the segmented data\n",
    "with open('./data1/sentences_data_series.pkl', 'rb') as file:\n",
    "    sentences_data_series = pickle.load(file)\n",
    "\n",
    "# Create a dataframe from the series data\n",
    "sentences_df = pd.DataFrame(sentences_data_series.tolist(), columns=['NoteID', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbc07c61-1b5c-4bd9-8876-2a69a7706d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can divide the sentence segmented data into smaller batches in order to make it computationally efficient\n",
    "# Here it takes the first 2m sentences\n",
    "subset_df = sentences_df.iloc[0:2000000].copy()\n",
    "# Make sure that there is no NaN cell\n",
    "subset_df = subset_df.fillna('None')\n",
    "\n",
    "print(subset_df.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "373fefa1-9296-4ea2-a008-cb0cb1c8bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the progress bar for pandas apply method\n",
    "tqdm.pandas(desc=\"Anonymizing text\")\n",
    "\n",
    "# Apply the anonymize function to the 'text' column and store the anonymized text in a new column 'Anonymized'\n",
    "subset_df['Anonymized'] = subset_df['text'].progress_apply(lambda x: anonymize(x, nlp)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad6f63c3-afdb-42bb-a087-2bf5fea88acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the segmented and anonymized batch into a pickle file to refer later\n",
    "subset_df.to_pickle('./data1/2023_notities_third_3.5m_sentences.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99fe3c7-bbef-48e9-b487-5e54c6c84859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
